# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# PyTorch
*.pth
*.pt
checkpoints/
lightning_logs/

# Data (dataset files only, not source code!)
/data/
tiny-imagenet-200/
*.zip

# Hydra
outputs/
multirun/
.hydra/

# Weights & Biases
wandb/

# IDE
.vscode/
.idea/
*.swp
*.swo
*~

# OS
.DS_Store
Thumbs.db

# Logs
logs/
*.log
*.out
*.err

# CUDA
*.o
*.cu.o

# =============================================================================
# TODO: COME BACK TO UNDERSTAND THESE CONCEPTS
# =============================================================================
# 1. Attention mechanism - specifically:
#    - What does "Patch A attends to Patch B with weight 0.15" mean?
#    - How does softmax(dim=-1) work on the attention matrix?
#    - Why does attn[batch, head, from_patch, to_patch] represent attention?
#    - File: src/model/layers.py, class Attention
# =============================================================================
